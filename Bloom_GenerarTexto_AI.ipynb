{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bloom_GenerarTexto.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIFjy2rhmofxsj5zk/WaBJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharlyPierce/TextToImagen/blob/main/Bloom_GenerarTexto_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Card Authors\n",
        "\n",
        "Ordered roughly chronologically and by amount of time spent.\n",
        "\n",
        "Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff\n",
        "\n",
        "[page](https://huggingface.co/bigscience/bloom)"
      ],
      "metadata": {
        "id": "ln5vFDKX4yht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BigScience, BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model. International, May 2021-May 2022"
      ],
      "metadata": {
        "id": "GCLhbuldABOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2zgAJd9yKm4",
        "outputId": "8358415f-d099-4094-d514-476ecda7afd4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug 27 23:02:20 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forma2\n"
      ],
      "metadata": {
        "id": "prcRaZQatyet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q##-q limpia la salida\n",
        "!pip install torch -q"
      ],
      "metadata": {
        "id": "cMHYcJkwtypX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd29ab69-4fe7-43d1-ea6a-8da648cc25b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.7 MB 34.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 65.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 59.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers \n",
        "import torch"
      ],
      "metadata": {
        "id": "yi0An8snbsjD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import BloomTokenizerFast\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b1\")# 1 billon de parametros\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-1b1\")\n",
        "#Pagina para ver modelos disponibles\n",
        "#https://huggingface.co/bigscience\n",
        "\n",
        "#En la ruta: https://huggingface.co/bigscience/bloom-1b7 en Technical Specifications aparece el No de parametros segun el modelo\n",
        "#bigscience/bloom-1b1  # 1 billon de parametros=1,065,314,304\n",
        "#bigscience/bloom-1b7  # 1 billon de parametros=1,722,408,960\n",
        "#bigscience/bloom-3b   # 3 billon de parametros=3,002,557,440\n",
        "#bigscience/bloom-7b1  # 7 billon de parametros=7,069,016,064\n",
        "\n",
        "\n",
        "#PLEASE DO NOT USE THIS MODEL! IT WILL BE REMOVED SOON. \n",
        "#USE https://huggingface.co/bigscience/bloom-1b1 instead which is the same model."
      ],
      "metadata": {
        "id": "ftpJC2JLolxq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"En esta escarpada tierra llamada Platea! Las ordas de Jerjes se enfrentan a la aniquilación! AUUU! Ahí están, los bárbaros desalmados, con el corazón encogido y\"\n",
        "lenn=200\n",
        "inputs=tokenizer(prompt,return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "81ZlwKpPgkj9"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample1=model.generate(inputs[\"input_ids\"],max_length=lenn,\n",
        "                                      do_sample=True,\n",
        "                                      top_k=50,\n",
        "                                      top_p=0.5,\n",
        "                                      repetition_penalty=1.5)\n",
        "#Parametros posibles para model.generate\n",
        "#sample = model.generate(**input_ids, max_length=200, num_beams = 2, num_beam_groups = 2, \n",
        "#                        top_k=1, temperature=0.9, repetition_penalty = 2.0, diversity_penalty = 0.9)\n",
        "#repetition_penalty = 2.0 sacado de tutorial.\n",
        "#\"https://www.youtube.com/watch?v=ZHx0TsYB3ac&t=930s\""
      ],
      "metadata": {
        "id": "A3obRwE-qhTH"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b760m=tokenizer.decode(sample1[0])\n",
        "cpl=75#chars_per_line\n",
        "print('Q:',prompt)\n",
        "[print(b760m[i:i+cpl]) for i in range(0,len(b760m),cpl) ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az-PTf-ihjJc",
        "outputId": "05ee7751-146f-4cf6-bcd9-de730ae4de78"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: En esta escarpada tierra llamada Platea! Las ordas de Jerjes se enfrentan a la aniquilación! AUUU! Ahí están, los bárbaros desalmados, con el corazón encogido y\n",
            "En esta escarpada tierra llamada Platea! Las ordas de Jerjes se enfrentan a\n",
            " la aniquilación! AUUU! Ahí están, los bárbaros desalmados, con el corazón \n",
            "encogido y sus ojos llorosos. ¿Pero qué ha pasado? ¿Qué es lo que han hecho\n",
            " para merecer esto?\n",
            "La verdad: No hay nada en absoluto; solo un puñado más \n",
            "grande del mismo tipo de monstruosidad salvaje.\n",
            "¿Lo entiendes ahora?! ¡El p\n",
            "oder corrompe todo!, ¿no te das cuenta…?, no tienes ninguna idea…\n",
            "¡Es por e\n",
            "so – porque Dios me envió al mundo como una prueba– he venido aquí sólo des\n",
            "pués haber sido tentado!\n",
            "Por favor veamos las pruebas reales cuando nos enc\n",
            "ontremos allí abajo. (Para aquellos que son conscientes)\n",
            "“Porque así dijo J\n",
            "ehová tu Señor : Yo soy mi paz ; nadie vendrá contra mí ni against mis hijo\n",
            "s” (Isaías 49:7).\n",
            "Ahora bien, si estas leyendo este\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None, None, None, None, None, None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf8lywANEwdS",
        "outputId": "00cfb777-73d1-4792-926c-eff70f40cec4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.generate.__doc__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-UA6izm8Mqc",
        "outputId": "d092330c-b1d8-4673-8c1e-89a0b2b3d605"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "        Generates sequences of token ids for models with a language modeling head. The method supports the following\n",
            "        generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:\n",
            "\n",
            "            - *greedy decoding* by calling [`~generation_utils.GenerationMixin.greedy_search`] if `num_beams=1` and\n",
            "              `do_sample=False`.\n",
            "            - *multinomial sampling* by calling [`~generation_utils.GenerationMixin.sample`] if `num_beams=1` and\n",
            "              `do_sample=True`.\n",
            "            - *beam-search decoding* by calling [`~generation_utils.GenerationMixin.beam_search`] if `num_beams>1` and\n",
            "              `do_sample=False`.\n",
            "            - *beam-search multinomial sampling* by calling [`~generation_utils.GenerationMixin.beam_sample`] if\n",
            "              `num_beams>1` and `do_sample=True`.\n",
            "            - *diverse beam-search decoding* by calling [`~generation_utils.GenerationMixin.group_beam_search`], if\n",
            "              `num_beams>1` and `num_beam_groups>1`.\n",
            "            - *constrained beam-search decoding* by calling\n",
            "              [`~generation_utils.GenerationMixin.constrained_beam_search`], if `constraints!=None` or\n",
            "              `force_words_ids!=None`.\n",
            "\n",
            "        <Tip warning={true}>\n",
            "\n",
            "        Apart from `inputs`, all the arguments below will default to the value of the attribute of the same name as\n",
            "        defined in the model's config (`config.json`) which in turn defaults to the\n",
            "        [`~modeling_utils.PretrainedConfig`] of the model.\n",
            "\n",
            "        </Tip>\n",
            "\n",
            "        Most of these parameters are explained in more detail in [this blog\n",
            "        post](https://huggingface.co/blog/how-to-generate).\n",
            "\n",
            "        Parameters:\n",
            "            inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
            "                The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
            "                method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
            "                should of in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
            "                `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
            "            max_length (`int`, *optional*, defaults to `model.config.max_length`):\n",
            "                The maximum length the generated tokens can have. Corresponds to the length of the input prompt +\n",
            "                `max_new_tokens`. In general, prefer the use of `max_new_tokens`, which ignores the number of tokens in\n",
            "                the prompt.\n",
            "            max_new_tokens (`int`, *optional*):\n",
            "                The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
            "            min_length (`int`, *optional*, defaults to 10):\n",
            "                The minimum length of the sequence to be generated.\n",
            "            do_sample (`bool`, *optional*, defaults to `False`):\n",
            "                Whether or not to use sampling ; use greedy decoding otherwise.\n",
            "            early_stopping (`bool`, *optional*, defaults to `False`):\n",
            "                Whether to stop the beam search when at least `num_beams` sentences are finished per batch or not.\n",
            "            num_beams (`int`, *optional*, defaults to 1):\n",
            "                Number of beams for beam search. 1 means no beam search.\n",
            "            temperature (`float`, *optional*, defaults to 1.0):\n",
            "                The value used to module the next token probabilities.\n",
            "            top_k (`int`, *optional*, defaults to 50):\n",
            "                The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
            "            top_p (`float`, *optional*, defaults to 1.0):\n",
            "                If set to float < 1, only the most probable tokens with probabilities that add up to `top_p` or higher\n",
            "                are kept for generation.\n",
            "            typical_p (`float`, *optional*, defaults to 1.0):\n",
            "                The amount of probability mass from the original distribution to be considered in typical decoding. If\n",
            "                set to 1.0 it takes no effect. See [this paper](https://arxiv.org/pdf/2202.00666.pdf) for more details.\n",
            "            repetition_penalty (`float`, *optional*, defaults to 1.0):\n",
            "                The parameter for repetition penalty. 1.0 means no penalty. See [this\n",
            "                paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\n",
            "            pad_token_id (`int`, *optional*):\n",
            "                The id of the *padding* token.\n",
            "            bos_token_id (`int`, *optional*):\n",
            "                The id of the *beginning-of-sequence* token.\n",
            "            eos_token_id (`int`, *optional*):\n",
            "                The id of the *end-of-sequence* token.\n",
            "            length_penalty (`float`, *optional*, defaults to 1.0):\n",
            "                 Exponential penalty to the length. 1.0 means that the beam score is penalized by the sequence length.\n",
            "                 0.0 means no penalty. Set to values < 0.0 in order to encourage the model to generate longer\n",
            "                 sequences, to a value > 0.0 in order to encourage the model to produce shorter sequences.\n",
            "            no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n",
            "                If set to int > 0, all ngrams of that size can only occur once.\n",
            "            encoder_no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n",
            "                If set to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot occur in the\n",
            "                `decoder_input_ids`.\n",
            "            bad_words_ids(`List[List[int]]`, *optional*):\n",
            "                List of token ids that are not allowed to be generated. In order to get the token ids of the words that\n",
            "                should not appear in the generated text, use `tokenizer(bad_words, add_prefix_space=True,\n",
            "                add_special_tokens=False).input_ids`.\n",
            "            force_words_ids(`List[List[int]]` or `List[List[List[int]]]`, *optional*):\n",
            "                List of token ids that must be generated. If given a `List[List[int]]`, this is treated as a simple\n",
            "                list of words that must be included, the opposite to `bad_words_ids`. If given `List[List[List[int]]]`,\n",
            "                this triggers a [disjunctive constraint](https://github.com/huggingface/transformers/issues/14081),\n",
            "                where one can allow different forms of each word.\n",
            "            num_return_sequences(`int`, *optional*, defaults to 1):\n",
            "                The number of independently computed returned sequences for each element in the batch.\n",
            "            max_time(`float`, *optional*):\n",
            "                The maximum amount of time you allow the computation to run for in seconds. generation will still\n",
            "                finish the current pass after allocated time has been passed.\n",
            "            attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
            "                Mask to avoid performing attention on padding token indices. Mask values are in `[0, 1]`, 1 for tokens\n",
            "                that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same shape\n",
            "                as `input_ids` that masks the pad token. [What are attention masks?](../glossary#attention-mask)\n",
            "            decoder_start_token_id (`int`, *optional*):\n",
            "                If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.\n",
            "            use_cache: (`bool`, *optional*, defaults to `True`):\n",
            "                Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
            "                speed up decoding.\n",
            "            num_beam_groups (`int`, *optional*, defaults to 1):\n",
            "                Number of groups to divide `num_beams` into in order to ensure diversity among different groups of\n",
            "                beams. [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.\n",
            "            diversity_penalty (`float`, *optional*, defaults to 0.0):\n",
            "                This value is subtracted from a beam's score if it generates a token same as any beam from other group\n",
            "                at a particular time. Note that `diversity_penalty` is only effective if `group beam search` is\n",
            "                enabled.\n",
            "            prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
            "                If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
            "                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
            "                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
            "                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
            "                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
            "                Retrieval](https://arxiv.org/abs/2010.00904).\n",
            "            logits_processor (`LogitsProcessorList`, *optional*):\n",
            "                 Custom logits processors that complement the default logits processors built from arguments and a\n",
            "                 model's config. If a logit processor is passed that is already created with the arguments or a model's\n",
            "                 config an error is thrown. This feature is intended for advanced users.\n",
            "            renormalize_logits: (`bool`, *optional*, defaults to `False`):\n",
            "                Whether to renormalize the logits after applying all the logits processors or warpers (including the\n",
            "                custom ones). It's highly recommended to set this flag to `True` as the search algorithms suppose the\n",
            "                score logits are normalized but some logit processors or warpers break the normalization.\n",
            "            stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
            "                 Custom stopping criteria that complement the default stopping criteria built from arguments and a\n",
            "                 model's config. If a stopping criteria is passed that is already created with the arguments or a\n",
            "                 model's config an error is thrown. This feature is intended for advanced users.\n",
            "            constraints (`List[Constraint]`, *optional*):\n",
            "                 Custom constraints that can be added to the generation to ensure that the output will contain the use\n",
            "                 of certain tokens as defined by `Constraint` objects, in the most sensible way possible.\n",
            "            output_attentions (`bool`, *optional*, defaults to `False`):\n",
            "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
            "                returned tensors for more details.\n",
            "            output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
            "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
            "                for more details.\n",
            "            output_scores (`bool`, *optional*, defaults to `False`):\n",
            "                Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
            "            return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
            "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            "            forced_bos_token_id (`int`, *optional*):\n",
            "                The id of the token to force as the first generated token after the `decoder_start_token_id`. Useful\n",
            "                for multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be\n",
            "                the target language token.\n",
            "            forced_eos_token_id (`int`, *optional*):\n",
            "                The id of the token to force as the last generated token when `max_length` is reached.\n",
            "            remove_invalid_values (`bool`, *optional*):\n",
            "                Whether to remove possible *nan* and *inf* outputs of the model to prevent the generation method to\n",
            "                crash. Note that using `remove_invalid_values` can slow down generation.\n",
            "            synced_gpus (`bool`, *optional*, defaults to `False`):\n",
            "                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
            "            exponential_decay_length_penalty (`tuple(int, float)`, *optional*):\n",
            "                This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been\n",
            "                generated. The tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates\n",
            "                where penalty starts and `decay_factor` represents the factor of exponential decay\n",
            "\n",
            "            model_kwargs:\n",
            "                Additional model specific kwargs will be forwarded to the `forward` function of the model. If the model\n",
            "                is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs\n",
            "                should be prefixed with *decoder_*.\n",
            "\n",
            "        Return:\n",
            "            [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
            "            or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n",
            "\n",
            "                If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
            "                [`~utils.ModelOutput`] types are:\n",
            "\n",
            "                    - [`~generation_utils.GreedySearchDecoderOnlyOutput`],\n",
            "                    - [`~generation_utils.SampleDecoderOnlyOutput`],\n",
            "                    - [`~generation_utils.BeamSearchDecoderOnlyOutput`],\n",
            "                    - [`~generation_utils.BeamSampleDecoderOnlyOutput`]\n",
            "\n",
            "                If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
            "                [`~utils.ModelOutput`] types are:\n",
            "\n",
            "                    - [`~generation_utils.GreedySearchEncoderDecoderOutput`],\n",
            "                    - [`~generation_utils.SampleEncoderDecoderOutput`],\n",
            "                    - [`~generation_utils.BeamSearchEncoderDecoderOutput`],\n",
            "                    - [`~generation_utils.BeamSampleEncoderDecoderOutput`]\n",
            "\n",
            "        Examples:\n",
            "\n",
            "        Greedy Decoding:\n",
            "\n",
            "        ```python\n",
            "        >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
            "\n",
            "        >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
            "        >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
            "\n",
            "        >>> prompt = \"Today I believe we can finally\"\n",
            "        >>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
            "\n",
            "        >>> # generate up to 30 tokens\n",
            "        >>> outputs = model.generate(input_ids, do_sample=False, max_length=30)\n",
            "        >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            "        ['Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n']\n",
            "        ```\n",
            "\n",
            "        Multinomial Sampling:\n",
            "\n",
            "        ```python\n",
            "        >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
            "        >>> import torch\n",
            "\n",
            "        >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
            "        >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
            "\n",
            "        >>> prompt = \"Today I believe we can finally\"\n",
            "        >>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
            "\n",
            "        >>> # sample up to 30 tokens\n",
            "        >>> torch.manual_seed(0)  # doctest: +IGNORE_RESULT\n",
            "        >>> outputs = model.generate(input_ids, do_sample=True, max_length=30)\n",
            "        >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            "        ['Today I believe we can finally get rid of discrimination,\" said Rep. Mark Pocan (D-Wis.).\\n\\n\"Just look at the']\n",
            "        ```\n",
            "\n",
            "        Beam-search decoding:\n",
            "\n",
            "        ```python\n",
            "        >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
            "\n",
            "        >>> tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
            "        >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
            "\n",
            "        >>> sentence = \"Paris is one of the densest populated areas in Europe.\"\n",
            "        >>> input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
            "\n",
            "        >>> outputs = model.generate(input_ids)\n",
            "        >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            "        ['Paris ist eines der dichtesten besiedelten Gebiete Europas.']\n",
            "        ```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "C-WJk3aVve1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___________________________________________________________________________________________________________________________"
      ],
      "metadata": {
        "id": "rx6RqU5Ove3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo que no funciono"
      ],
      "metadata": {
        "id": "LW78RgnyuNwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tutorial base: \"https://www.youtube.com/watch?v=rrZGIR5CryM&t=28s\"\n",
        "\n",
        "#from transformers import BloomForCasualLM#Se ocupa para model\n",
        "#Manda Error linea de arriba\n",
        "#Cambiamos linea de arriba por\n",
        "from transformers import  AutoModel#Pagina oficial indica esto\n",
        "from transformers import AutoModelForCausalLM#Tambien por esto\n",
        "#Tutorial indica esto:AutoModelForCausalLM\n",
        "#\"https://www.youtube.com/watch?v=ZHx0TsYB3ac&t=930s\"\n",
        "\n",
        "#Al parecer no se ocupa\n",
        "#from transformers import BloomForTokenClassification\n",
        "from transformers import BloomTokenizerFast\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-760m\")\n",
        "#Al usar AutoModel produce un error por causa de los parametros.\n",
        "#model = AutoModel.from_pretrained(\"bigscience/bloom-760m\")\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-760m\")"
      ],
      "metadata": {
        "id": "PuX98CiouPbZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}